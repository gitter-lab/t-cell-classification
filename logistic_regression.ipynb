{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook, we implement three logistic regression classifiers with different features. All models are using $L_1$ penalty, and the regularization power is tuned using the Nested Cross-Validation scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nested Cross-Validation Scheme\n",
    "\n",
    "For all models having hyper-parameters, we use the same Nested Cross-Validation method to tune, train and test models. We iterate test donor from donor 1, 2, 3, 5 and 6. Each test donor is separated from the tuning and training process of its associated model. During tuning, the validation set is iterated through the left 4 donors. The average of performances on 4 validation sets is used to determine the best parameter for one test donor. Therefore, each test donor might have different optimal parameters.\n",
    "\n",
    "<img src=\"./plots/cross_validation.pdf\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression with Image Pixel\n",
    "\n",
    "### 2.1. Data Preparation\n",
    "\n",
    "For the first logistic regression model, we use the $78\\times 78$ pixel matrix as the representation of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from os.path import basename, join, exists\n",
    "from json import load, dump\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "\n",
    "# Load the pixel matrix for each donor\n",
    "for d in [1, 2, 3, 5, 6]:\n",
    "    no_aug_x, no_aug_y = [], []\n",
    "    aug_x, aug_y = [], []\n",
    "    \n",
    "    for name in glob(\"./images/sample_images/processed/augmented/donor_{}/*/*.png\".format(d)):\n",
    "        pixel_mat = cv2.imread(name, 0)\n",
    "        pixel_feature = pixel_mat.reshape((1, -1))\n",
    "        \n",
    "        # Add the feature and label to the correct list\n",
    "        base_name = basename(name)\n",
    "        cur_label = 0 if 'noact' in base_name else 1\n",
    "        \n",
    "        if 'r' not in base_name and 'f' not in base_name:\n",
    "            no_aug_x.append(pixel_feature)\n",
    "            no_aug_y.append(cur_label)\n",
    "        \n",
    "        aug_x.append(pixel_feature)\n",
    "        aug_y.append(cur_label)\n",
    "\n",
    "    assert(len(aug_x) == len(aug_y))\n",
    "    assert(len(no_aug_x) == len(no_aug_x))\n",
    "    \n",
    "    # Add data for this donor\n",
    "    all_data[d] = {\n",
    "        'no_aug_x': np.vstack(no_aug_x),\n",
    "        'no_aug_y': np.array(no_aug_y),\n",
    "        'aug_x': np.vstack(aug_x),\n",
    "        'aug_y': np.array(aug_y)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_aug_x': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'no_aug_y': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0]),\n",
       " 'aug_x': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'aug_y': array([1, 1, 1, ..., 0, 0, 0])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Model Tuning/Training\n",
    "\n",
    "After setting up the training data, we can use Nested Cross-Validation to tune the regularization parameter for each test donor. We use grid search to try all $\\lambda$ candidates $[0.0001, 0.001, 0.01, 0.1, 1]$. We use average precision to choose the best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv(all_data, c_candidates):\n",
    "    \"\"\"\n",
    "    Use nested cross validation to find the best parameter\n",
    "    for each test donor.\n",
    "    \n",
    "    Args:\n",
    "        all_data(dict): mapping each donor to four lists: augmented features,\n",
    "            augmented labels, un-augmented features, un-augmented labels\n",
    "        c_candidates(list): the candidate for lambda value\n",
    "        \n",
    "    Return:\n",
    "        dict: mapping each donor to its best lambda value\n",
    "    \"\"\"\n",
    "\n",
    "    best_parameters = {}\n",
    "    for tid in [1, 2, 3, 5, 6]:\n",
    "\n",
    "        # Validate 4 times for each parameter\n",
    "        vids = [i for i in [1, 2, 3, 5, 6] if i != tid]\n",
    "\n",
    "        # This array mirrors the same index of c_candidates\n",
    "        average_aps = np.zeros(len(c_candidates))\n",
    "\n",
    "        for i in range(len(c_candidates)):\n",
    "            c = c_candidates[i]\n",
    "            lr_model = LogisticRegression(penalty='l1', C=c, class_weight='balanced')\n",
    "            aps = []\n",
    "\n",
    "            for vid in vids:\n",
    "                # Training set includes augmented images from {all donor - test donor - vali donor}\n",
    "                train_x = np.vstack([all_data[i]['aug_x'] for i in [1, 2, 3, 5, 6]\n",
    "                                     if i != tid and i != vid])\n",
    "                train_y = np.hstack([all_data[i]['aug_y'] for i in [1, 2, 3, 5, 6]\n",
    "                                     if i != tid and i != vid])\n",
    "                assert(train_x.shape[0] == train_y.shape[0])\n",
    "\n",
    "                # Validation set includes non-augmented images from {vali donor}\n",
    "                vali_x = all_data[vid]['aug_x']\n",
    "                vali_y = all_data[vid]['aug_y']\n",
    "\n",
    "                # Train the model on the trining set\n",
    "                lr_model.fit(train_x, train_y)\n",
    "\n",
    "                # Test the model on the validation set\n",
    "                predicted_score = lr_model.predict_proba(vali_x)\n",
    "\n",
    "                # Many metric functions in sklearn requires 1d score array (pos proba)\n",
    "                predicted_score_1d = [s[1] for s in predicted_score]\n",
    "\n",
    "                # Compute the average precision for the current validation\n",
    "                aps.append(metrics.average_precision_score(vali_y, predicted_score_1d))\n",
    "\n",
    "            # Compute the average of 4 validation runs\n",
    "            average_aps[i] = np.mean(aps)\n",
    "            print(\"The average performance of parameter {} for test donor {} is {:.4f}.\".format(\n",
    "                c, tid, np.mean(aps)\n",
    "            ))\n",
    "\n",
    "        # Now, we have exhaustively considered all parameter candidates. Choose the one giving the\n",
    "        # best mean average precision score\n",
    "        current_best_para = c_candidates[np.argmax(average_aps)]\n",
    "        print(\"The best parameter for test donor {} is {}\".format(tid, current_best_para))\n",
    "        best_parameters[tid] = current_best_para\n",
    "    \n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average performance of parameter 0.0001 for test donor 1 is 0.8697.\n",
      "The average performance of parameter 0.001 for test donor 1 is 0.8773.\n",
      "The average performance of parameter 0.01 for test donor 1 is 0.8254.\n",
      "The average performance of parameter 0.1 for test donor 1 is 0.8039.\n",
      "The average performance of parameter 1 for test donor 1 is 0.7587.\n",
      "The best parameter for test donor 1 is 0.001\n",
      "The average performance of parameter 0.0001 for test donor 2 is 0.7590.\n",
      "The average performance of parameter 0.001 for test donor 2 is 0.7807.\n",
      "The average performance of parameter 0.01 for test donor 2 is 0.7191.\n",
      "The average performance of parameter 0.1 for test donor 2 is 0.6788.\n",
      "The average performance of parameter 1 for test donor 2 is 0.6137.\n",
      "The best parameter for test donor 2 is 0.001\n",
      "The average performance of parameter 0.0001 for test donor 3 is 0.7685.\n",
      "The average performance of parameter 0.001 for test donor 3 is 0.8133.\n",
      "The average performance of parameter 0.01 for test donor 3 is 0.7455.\n",
      "The average performance of parameter 0.1 for test donor 3 is 0.7292.\n",
      "The average performance of parameter 1 for test donor 3 is 0.6903.\n",
      "The best parameter for test donor 3 is 0.001\n",
      "The average performance of parameter 0.0001 for test donor 5 is 0.7492.\n",
      "The average performance of parameter 0.001 for test donor 5 is 0.7772.\n",
      "The average performance of parameter 0.01 for test donor 5 is 0.6854.\n",
      "The average performance of parameter 0.1 for test donor 5 is 0.6551.\n",
      "The average performance of parameter 1 for test donor 5 is 0.6060.\n",
      "The best parameter for test donor 5 is 0.001\n",
      "The average performance of parameter 0.0001 for test donor 6 is 0.7246.\n",
      "The average performance of parameter 0.001 for test donor 6 is 0.7749.\n",
      "The average performance of parameter 0.01 for test donor 6 is 0.7258.\n",
      "The average performance of parameter 0.1 for test donor 6 is 0.7160.\n",
      "The average performance of parameter 1 for test donor 6 is 0.6776.\n",
      "The best parameter for test donor 6 is 0.001\n"
     ]
    }
   ],
   "source": [
    "# Smaller c gives stronger regularization power\n",
    "c_candidates = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "best_parameters = nested_cv(all_data, c_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.001, 2: 0.001, 3: 0.001, 5: 0.001, 6: 0.001}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the model, we found $0.001$ is the best $\\lambda$ for all 5 test donors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Model Testing\n",
    "\n",
    "Then, we can use the best parameter to train a new model on all augmented images in the new training set, and test it on un-augmented images from each test donor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, x_test, y_test, pos=1):\n",
    "    \"\"\"\n",
    "    This funciton runs the trained `model` on `x_test`, compares the test\n",
    "    result with `y_test`. Finally, it outputs a collection of various\n",
    "    classificaiton performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: a trained sklearn model\n",
    "        x_test(np.array(m, n)): 2D feature array in the testset, m elements and each\n",
    "            element has n features\n",
    "        y_test(np.array(m)): 1D label array in the testset. There are m entries.\n",
    "        pos: the encoding of postive label in `y_test`\n",
    "\n",
    "    Return:\n",
    "        A dictionary containing the metrics information and predictions:\n",
    "            metrics scores: ['acc': accuracy, 'precision', 'recall', 'ap': average precision,\n",
    "                             'aroc': area under ROC curve, 'pr': PR curve points,\n",
    "                             'roc': ROC curve points]\n",
    "            predicitons: ['y_true': the groundtruth labels, 'y_score': predicted probability]\n",
    "    \"\"\"\n",
    "\n",
    "    y_predict_prob = model.predict_proba(x_test)\n",
    "    y_predict = model.predict(x_test)\n",
    "\n",
    "    # Sklearn requires the prob list to be 1D\n",
    "    y_predict_prob = [x[pos] for x in y_predict_prob]\n",
    "    y_test_fixed = np.array(y_test)\n",
    "    \n",
    "    if pos == 0:\n",
    "        # Flip the array so 1 represents the positive class\n",
    "        y_test_fixed = 1 - np.array(y_test)\n",
    "\n",
    "    # Compute the PR-curve points\n",
    "    precisions, recalls, thresholds = metrics.precision_recall_curve(\n",
    "        y_test_fixed,\n",
    "        y_predict_prob,\n",
    "        pos_label=pos\n",
    "    )\n",
    "\n",
    "    # Compute the roc-curve points\n",
    "    fprs, tprs, roc_thresholds = metrics.roc_curve(y_test_fixed, y_predict_prob,\n",
    "                                                   pos_label=pos)\n",
    "\n",
    "    return ({'acc': metrics.accuracy_score(y_test_fixed, y_predict),\n",
    "             'precision': metrics.precision_score(y_test_fixed, y_predict,\n",
    "                                                  pos_label=pos),\n",
    "             'recall': metrics.recall_score(y_test_fixed, y_predict,\n",
    "                                            pos_label=pos),\n",
    "             'ap': metrics.average_precision_score(y_test_fixed,\n",
    "                                                   y_predict_prob),\n",
    "             'aroc': metrics.roc_auc_score(y_test_fixed,\n",
    "                                           y_predict_prob),\n",
    "             'pr': [precisions.tolist(), recalls.tolist(),\n",
    "                    thresholds.tolist()],\n",
    "             'roc': [fprs.tolist(), tprs.tolist(), roc_thresholds.tolist()],\n",
    "             'y_true': y_test,\n",
    "             'y_score': y_predict_prob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(metric_dict, count_dict):\n",
    "    \"\"\"\n",
    "    Transfer the model performance metric dictionary into a Markdown table.\n",
    "    \n",
    "    Args:\n",
    "        metric_dict(dict): a dictionary encoding model performance statisitcs\n",
    "            and prediction information for all test donors\n",
    "        count_dict(dict): a dinctionary encoding the count of activated and quiescent\n",
    "            samples for each test donor\n",
    "    \n",
    "    Return:\n",
    "        string: a Markdown syntax table\n",
    "    \"\"\"\n",
    "\n",
    "    # Define header and line template\n",
    "    table_str = \"\"\n",
    "    line = \"|{}|{:.2f}%|{:.2f}%|{:.2f}%|{:.2f}%|{:.2f}%|{}|{}|\\n\"\n",
    "    table_str += (\"|Test Donor|Accuracy|Precision|Recall|Average Precision|AUC|Num of Activated|Num of Quiescent|\\n\")\n",
    "    table_str += \"|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\\n\"\n",
    "\n",
    "    for d in [1, 2, 3, 5, 6]:\n",
    "        result = metric_dict[d]\n",
    "        table_str += (line.format(\"donor_{}\".format(d),\n",
    "                                  result['acc'] * 100, result['precision'] * 100,\n",
    "                                  result['recall'] * 100, result['ap'] * 100,\n",
    "                                  result['aroc'] * 100, count_dict[d]['activated'],\n",
    "                                  count_dict[d]['quiescent']))\n",
    "\n",
    "    return table_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = {}\n",
    "\n",
    "for tid in [1, 2, 3, 5, 6]:\n",
    "    # Collect the performance metrics for each test donor tid\n",
    "    lr_model = LogisticRegression(penalty='l1', C=best_parameters[d], class_weight='balanced')\n",
    "    \n",
    "    # Training set includes augmented images from {all donor - test donor}\n",
    "    train_x = np.vstack([training_data[i]['aug_x'] for i in [1, 2, 3, 5, 6] if i != tid])\n",
    "    train_y = np.hstack([training_data[i]['aug_y'] for i in [1, 2, 3, 5, 6] if i != tid])\n",
    "    assert(train_x.shape[0] == train_y.shape[0])\n",
    "\n",
    "    # Test set includes non-augmented images from {test donor}\n",
    "    test_x = training_data[tid]['no_aug_x']\n",
    "    test_y = training_data[tid]['no_aug_y']\n",
    "    \n",
    "    lr_model.fit(train_x, train_y)\n",
    "    \n",
    "    model_performance[tid] = get_score(lr_model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the lables for each donor\n",
    "count_dict = {}\n",
    "\n",
    "for d in [1, 2, 3, 5, 6]:\n",
    "    act_count = len(glob(\"./images/sample_images/processed/augmented/donor_{}/activated/*.png\".format(d)))\n",
    "    qui_count = len(glob(\"./images/sample_images/processed/augmented/donor_{}/quiescent/*.png\".format(d)))\n",
    "    count_dict[d] = {\n",
    "        'activated': act_count,\n",
    "        'quiescent': qui_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|Test Donor|Accuracy|Precision|Recall|Average Precision|AUC|Num of Activated|Num of Quiescent|\n",
       "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
       "|donor_1|65.00%|21.92%|72.73%|46.30%|73.82%|132|948|\n",
       "|donor_2|60.00%|92.31%|55.38%|92.45%|71.18%|390|90|\n",
       "|donor_3|70.76%|47.62%|86.96%|78.57%|86.85%|276|750|\n",
       "|donor_5|67.39%|84.91%|67.16%|90.55%|74.45%|402|150|\n",
       "|donor_6|81.37%|72.73%|90.91%|89.39%|89.77%|264|348|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a table summary\n",
    "display(Markdown(make_table(model_performance, count_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the average accuracy for all test donors are above $50\\%$. A simple logistic regression model with basic feature encoding can give better predictions than random guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression with Image Size and Total Intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Preparation\n",
    "\n",
    "### 3.2. Model Tuning/Training\n",
    "\n",
    "### 3.3. Model Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression with CellProfiler Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Data Preparation\n",
    "\n",
    "### 4.2. Model Tuning/Training\n",
    "\n",
    "### 4.3. Model Testing\n",
    "\n",
    "### 4.4. Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
